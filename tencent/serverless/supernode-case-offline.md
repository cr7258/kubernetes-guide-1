# 超级节点案例分享: 便捷管理离线任务与大规模压测

## 概述

腾讯云容器服务的超级节点有着隔离性强，扩容快，成本低等特点，天然适合离线任务与大规模压测。

本文分享这种场景的几个真实实践案例。

## 案例一: CI 系统(某出行客户)

gitlab-runner 启动 Pod 运行 CI 任务，任务结束即销毁 Pod，使用常驻节点会造成资源利用率低。任务量大时扩容节点时间长，造成部分 CI 任务过慢。

方案改进: 使用 Serverless 集群(超级节点)，无需常驻节点资源，Pod 按量计费，且支持竞价实例，任务结束即停止计费，降低成本。任务量大时也可以快速扩容，提高 CI 效率。

![](https://image-host-1251893006.cos.ap-chengdu.myqcloud.com/超级节点CI案例.png)

## 案例二: 游戏 AI 训练(某游戏客户)

使用 GPU Pod 训练游戏 NPC AI 模型，训练完成后，再启动大量 CPU Pod 对模型进行验证。

![](https://image-host-1251893006.cos.ap-chengdu.myqcloud.com/超级节点游戏AI训练案例.png)

使用 TKE 普通节点，持续跑大量任务，Pod 数量规模巨大且扩缩容频繁，导致普通节点经常需要扩容。普通节点扩容慢，导致部分任务过慢。扩容过程可能出错，比如售罄，初始化失败等。

方案改进: 切换到 Serverless 集群(超级节点)，扩缩容速度得到极大提升(超10倍)，不再有任务过慢的情况。由于使用超级节点，购买的资源规格取决于 Pod 规格，没有大规格，不容易出现售罄；没有初始化节点过程，也不会发生初始化失败的问题。超级节点支持 Pod 的竞价实例，且任务跑完即释放，极大降低成本。

## 案例三: 大规模 CronJob 优化 (某教育客户)

因业务需要，需要启动大规模的 CronJob 跑离线任务，使用 TKE 普通节点，在线业务与离线 CronJob 混部，频繁启停场景下，cgroup 弱隔离带来普通节点稳定性问题。为避免售罄、节点扩容慢问题，购买了大量包年包月常驻节点，低峰期资源利用率低很低。

方案改进: 添加超级节点，将 CronJob 调度到超级节点，普通节点稳定性大幅提升。无需预留资源，pod 按量计费，定时任务资源成本降低 70% 左右。Job 实现秒级启动（EKS 镜像缓存，pod 启动加速）。

![](https://image-host-1251893006.cos.ap-chengdu.myqcloud.com/超级节点cronjob案例.png)

## 案例四: 边缘集群直播案例 (某视频客户)

问题与困境: 在中心地域部署业务，边缘主播推流延迟大影响体验。每个地域都单独部署一套 K8S 集群，运维压力大。

Serverless 集群方案: 统一 K8S 接口运维多地域集群，无节点，免运维。弹性转码服务，成本低，扩容灵活。

![](https://image-host-1251893006.cos.ap-chengdu.myqcloud.com/20220719190209.png)

## 案例五: 日志处理 (某社交平台客户)

使用  logstash 进行日志清洗，集群规模大，业务高峰期产生日志量特别大，普通节点扩容慢，导致有丢日志的情况发生。高峰期过后，普通节点资源利用率较低。

方案改进: 高峰期极速扩容，不存在丢日志问题。高峰期过后，平均负载降低，自动缩容，缩掉的 Pod 停止计费，提高资源利用率，降低成本。

![](https://image-host-1251893006.cos.ap-chengdu.myqcloud.com/超级节点日志清洗案例.png)

## 案例六: 大规模压测 (某社交平台客户)

TKE 普通节点隔离性弱，压测时需要控制调度策略，避免与在线业务混部，造成干扰。压测时带宽消耗非常大，单节点调度过多压测 Pod 容易达到节点带宽瓶颈而丢包。

方案改进: 使用 Serverless 集群(超级节点)， Pod 之间强隔离，压测 Pod 不会对在线业务造成干扰，无需关心调度策略，解放运维。每个 Pod 独占虚拟机，基本不会因达到带宽瓶颈而丢包。

![](https://image-host-1251893006.cos.ap-chengdu.myqcloud.com/超级节点大规模压测案例.png)